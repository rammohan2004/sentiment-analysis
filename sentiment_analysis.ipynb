{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6db52c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches as mpatches\n",
    "import seaborn as sns\n",
    "\n",
    "class DataVisualizer:\n",
    "\tdef plotConfusionMatrix(self, cm, labels, clf_name):\n",
    "\t\tfig, ax = plt.subplots()\n",
    "\t\tsns.heatmap(cm, annot=True, ax = ax, fmt = 'g')\n",
    "\n",
    "\t\tax.set_xlabel('Predicted labels')\n",
    "\t\tax.set_ylabel('Actual labels')\n",
    "\t\tax.set_title('Confusion Matrix of {} Classifier'.format(clf_name))\n",
    "\t\tax.xaxis.set_ticklabels(labels)\n",
    "\t\tax.yaxis.set_ticklabels(labels, rotation = 0)\n",
    "\t\tplt.tight_layout()\n",
    "\t\tfig.savefig('plots/cm_{}.png'.format(clf_name.lower().replace(' ', '_')))\n",
    "\t\tplt.close()\n",
    "\n",
    "\tdef plotClassificationReport(self, cr, labels, clf_name):\n",
    "\t\tcr_mat = []\n",
    "\t\tallowed_labels = ['negative', 'positive', 'weighted avg']\n",
    "\n",
    "\t\tlines = cr.split('\\n')\n",
    "\t\tfor line in lines[2 : -1]:\n",
    "\t\t\tline = line.strip()\n",
    "\t\t\tif line == '':\n",
    "\t\t\t\tcontinue\n",
    "\t\t\trow = re.split(r'\\s{2,}', line)\n",
    "\t\t\tif row[0] not in allowed_labels:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\trow_data = []\n",
    "\t\t\trow_data.append(float(row[-4]))\n",
    "\t\t\trow_data.append(float(row[-3]))\n",
    "\t\t\trow_data.append(float(row[-2]))\n",
    "\t\t\trow_data.append(float(row[-1]))\n",
    "\t\t\tcr_mat.append(row_data)\n",
    "\n",
    "\t\txlabels = ['precision', 'recall', 'f1-score', 'support']\n",
    "\t\tylabels = labels + ['weighted avg']\n",
    "\n",
    "\t\tfig, ax = plt.subplots()\n",
    "\t\tsns.heatmap(cr_mat, annot = True, ax = ax, fmt = 'g')\n",
    "\n",
    "\t\tax.set_xlabel('Metrics')\n",
    "\t\tax.set_ylabel('Classes')\n",
    "\t\tax.set_title('Classification Report of {} Classifier'.format(clf_name))\n",
    "\t\tax.xaxis.set_ticklabels(xlabels)\n",
    "\t\tax.yaxis.set_ticklabels(ylabels, rotation = 0)\n",
    "\t\tplt.tight_layout()\n",
    "\t\tfig.savefig('plots/cr_{}.png'.format(clf_name.lower().replace(' ', '_')))\n",
    "\t\tplt.close()\n",
    "\n",
    "\tdef plotClassifierPerformanceComparison(self, metrics_df, clf_names, strategy):\n",
    "\t\tfig, ax = plt.subplots()\n",
    "\t\tsns.barplot(x = 'Metrics', y = 'value', data = metrics_df, ax = ax, hue = 'Classifier')\n",
    "\n",
    "\t\tax.set_xlabel('Evaluation Metrics')\n",
    "\t\tax.set_ylabel('Classifier\\'s performance')\n",
    "\t\tax.set_title('Overall Comparison of Classifier\\'s Performance (' + strategy + ')')\n",
    "\t\tpos = ax.get_position()\n",
    "\t\tax.set_position([pos.x0, pos.y0, pos.width, pos.height])\n",
    "\t\tplt.legend(bbox_to_anchor = (1, 0.5), loc = 'best')\n",
    "\t\tplt.tight_layout()\n",
    "\t\tif strategy == 'K-Fold':\n",
    "\t\t\tfig.savefig('plots/classifiers_vs_metrics_kfold.png')\n",
    "\t\telse:\n",
    "\t\t\tfig.savefig('plots/classifiers_vs_metrics.png')\n",
    "\t\tplt.close()\n",
    "\n",
    "\tdef plotClassifiersVsFeatures(self, data, clf_names, colors):\n",
    "\t\tfig, ax = plt.subplots()\n",
    "\t\tlines = []\n",
    "\t\tfor d, c, clf_name in zip(data, colors, clf_names):\n",
    "\t\t\tsns.pointplot(x = 'x', y = 'y', data = d, ax = ax, color = c)\n",
    "\t\t\tlines.append(mpatches.Patch(color = c, label = clf_name))\n",
    "\n",
    "\t\tax.legend(handles = lines, bbox_to_anchor=(1, 0.5), loc = 'best')\n",
    "\t\tax.set_xlabel('K-Best Features')\n",
    "\t\tax.set_ylabel('Classification Accuracy Scores')\n",
    "\t\tax.set_title('Comparison of Classifier\\'s Performance over Selected Features')\n",
    "\t\tfig.savefig('plots/classifiers_vs_features.png')\n",
    "\t\tplt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e37a063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Rammohan\n",
      "[nltk_data]     rao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "class NltkPreprocessor:\n",
    "\n",
    "\tdef __init__(self, stopwords = None, punct = None, lower = True, strip = True):\n",
    "\t\tself.lower = lower\n",
    "\t\tself.strip = strip\n",
    "\t\tself.stopwords =  set(sw.words('english'))\n",
    "\t\tself.punct =  set(string.punctuation)\n",
    "\t\tself.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\tdef tokenize(self, document):\n",
    "\t\ttokenized_doc = []\n",
    "\t\tfor sent in sent_tokenize(document):\n",
    "\t\t\tfor token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "\t\t\t\ttoken = token.lower() if self.lower else token\n",
    "\t\t\t\ttoken = token.strip() if self.strip else token\n",
    "\t\t\t\ttoken = token.strip('_0123456789') if self.strip else token\n",
    "\n",
    "\t\t\t\tif token in self.stopwords:\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\tif all(char in self.punct for char in token):\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\tlemma = self.lemmatize(token, tag)\n",
    "\t\t\t\ttokenized_doc.append(lemma)\n",
    "\n",
    "\t\treturn ' '.join(tokenized_doc)\n",
    "\n",
    "\tdef lemmatize(self, token, tag):\n",
    "\t\ttag = {\n",
    "\t\t\t'N': wn.NOUN,\n",
    "\t\t\t'V': wn.VERB,\n",
    "\t\t\t'R': wn.ADV,\n",
    "\t\t\t'J': wn.ADJ\n",
    "\t\t}.get(tag[0], wn.NOUN)\n",
    "\n",
    "\t\treturn self.lemmatizer.lemmatize(token, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136589c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import ast\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tself.clf = [\n",
    "\t\t\t('MNB', MultinomialNB(alpha = 1.0, fit_prior = False)),\n",
    "\t\t\t('LR', LogisticRegression(C = 5.0, penalty = 'l2', solver = 'liblinear', max_iter = 100, dual = True)),\n",
    "\t\t\t('SVM', LinearSVC(C = 0.55, penalty = 'l2', max_iter = 1000, dual = True)),\n",
    "\t\t\t('RF', RandomForestClassifier(n_jobs = -1, n_estimators = 100, min_samples_split = 40, max_depth = 90, min_samples_leaf = 3))\n",
    "\t\t]\n",
    "\t\tself.clf_names = ['Multinomial NB', 'Logistic Regression', 'Linear SVC', 'Random Forest']\n",
    "\n",
    "\tdef getInitialData(self, data_file, do_pickle):\n",
    "\t\tprint('Fetching initial data...')\n",
    "\t\tt = time()\n",
    "\n",
    "\t\ti = 0\n",
    "\t\tdf = {}\n",
    "\t\twith open(data_file, 'r') as file_handler:\n",
    "\t\t\tfor review in file_handler.readlines():\n",
    "\t\t\t\tdf[i] = ast.literal_eval(review)\n",
    "\t\t\t\ti += 1\n",
    "\n",
    "\t\treviews_df = pd.DataFrame.from_dict(df, orient = 'index')\n",
    "\t\tif do_pickle:\n",
    "\t\t\treviews_df.to_pickle('pickled/product_reviews.pickle')\n",
    "\n",
    "\t\tprint('Fetching data completed!')\n",
    "\t\tprint('Fetching time: ', round(time()-t, 3), 's\\n')\n",
    "\n",
    "\tdef preprocessData(self, reviews_df, do_pickle):\n",
    "\t\tprint('Preprocessing data...')\n",
    "\t\tt = time()\n",
    "\n",
    "\t\treviews_df.drop(columns = ['reviewSummary'], inplace = True)\n",
    "\t\treviews_df['reviewRating'] = reviews_df.reviewRating.astype('int')\n",
    "\n",
    "\t\treviews_df = reviews_df[reviews_df.reviewRating != 3] # Ignoring 3-star reviews -> neutral\n",
    "        \n",
    "\t\treviews_df = reviews_df.assign(sentiment = np.where(reviews_df['reviewRating'] >= 4, 1, 0)) # 1 -> Positive, 0 -> Negative\n",
    "        \n",
    "\t\tnltk_preprocessor = NltkPreprocessor()\n",
    "        \n",
    "\t\twith mp.Pool() as pool:\n",
    "\t\t\treviews_df = reviews_df.assign(cleaned = pool.map(nltk_preprocessor.tokenize, reviews_df['reviewText'])) # Parallel processing\n",
    "\t\t\n",
    "\t\tif do_pickle:\n",
    "\t\t\treviews_df.to_pickle('pickled/product_reviews_preprocessed.pickle')\n",
    "\n",
    "\t\tprint('Preprocessing data completed!')\n",
    "\t\tprint('Preprocessing time: ', round(time()-t, 3), 's\\n')\n",
    "\n",
    "\tdef trainTestSplit(self, reviews_df_preprocessed):\n",
    "\t\tprint('Splitting data using Train-Test split...')\n",
    "\t\tt = time()\n",
    "\t\t\n",
    "\t\tX = reviews_df_preprocessed.iloc[:, -1].values\n",
    "\t\ty = reviews_df_preprocessed.iloc[:, -2].values\n",
    "\n",
    "\t\tX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, shuffle = True)\n",
    "\n",
    "\t\tprint('Splitting data completed!')\n",
    "\t\tprint('Splitting time: ', round(time()-t, 3), 's\\n')\n",
    "\n",
    "\t\treturn X_train, X_test, y_train, y_test\n",
    "\n",
    "\tdef kFoldSplit(self, reviews_df_preprocessed):\n",
    "\t\tprint('Splitting data using K-Fold Cross Validation...')\n",
    "\t\tt = time()\n",
    "\t\t\n",
    "\t\tX = reviews_df_preprocessed.iloc[:, -1].values\n",
    "\t\ty = reviews_df_preprocessed.iloc[:, -2].values\n",
    "\n",
    "\t\tkf = KFold(n_splits = 5, random_state = 42, shuffle = True)\n",
    "\t\ttrain_test_indices = kf.split(X, y)\n",
    "\n",
    "\t\tprint('Splitting data completed!')\n",
    "\t\tprint('Splitting time: ', round(time()-t, 3), 's\\n')\n",
    "\n",
    "\t\treturn train_test_indices, X, y\n",
    "\n",
    "\tdef trainData(self, X_train, y_train, classifier, num_features = 1000000):\n",
    "\t\tpipeline = []\n",
    "\t\tmodel = []\n",
    "\n",
    "\t\tsteps = [\n",
    "\t\t\t\t\t('vect', TfidfVectorizer(ngram_range = (1,2), use_idf = True, sublinear_tf = True, lowercase = False, stop_words = None, preprocessor = None)),\n",
    "\t\t\t\t\t('select_best', SelectKBest(score_func = chi2, k = num_features))\n",
    "\t\t\t\t]\n",
    "\n",
    "\t\tfor name, clf in classifier:\n",
    "\t\t\tsteps.append(('clf', clf))\n",
    "\t\t\tpl = Pipeline(steps)\n",
    "\t\t\tpipeline.append(pl)\n",
    "\n",
    "\t\t\tprint('Training data... Classifier ' + str(name))\n",
    "\t\t\tt = time()\n",
    "\n",
    "\t\t\tmodel.append((name, pl.fit(X_train, y_train)))\n",
    "\n",
    "\t\t\tprint('Training data completed!')\n",
    "\t\t\tprint('Training time: ', round(time()-t, 3), 's\\n')\n",
    "\n",
    "\t\t\tsteps.pop()\n",
    "\n",
    "\t\treturn pipeline, model\n",
    "\n",
    "\tdef predictData(self, X_test, model):\n",
    "\t\tprediction = []\n",
    "\n",
    "\t\tfor name, m in model:\n",
    "\t\t\tprint('Predicting Test data... Classifier ' + str(name))\n",
    "\t\t\tt = time()\n",
    "\n",
    "\t\t\tprediction.append((name, m.predict(X_test)))\n",
    "\n",
    "\t\t\tprint('Prediction completed!')\n",
    "\t\t\tprint('Prediction time: ', round(time()-t, 3), 's\\n')\n",
    "\n",
    "\t\treturn prediction\n",
    "\n",
    "\tdef evaluate(self, y_test, prediction):\n",
    "\t\tclf_accuracy = []\n",
    "\t\tclf_precision = []\n",
    "\t\tclf_recall = []\n",
    "\t\tclf_f1 = []\n",
    "\t\tclf_roc_auc = []\n",
    "\t\tclf_cm = []\n",
    "\t\tclf_cr = []\n",
    "\t\t\n",
    "\t\tfor name, pred in prediction:\n",
    "\t\t\tprint('Evaluating results... Classifier ' + str(name))\n",
    "\t\t\tt = time()\n",
    "\n",
    "\t\t\tclf_accuracy.append(accuracy_score(y_test, pred))\n",
    "\t\t\tclf_precision.append(precision_score(y_test, pred))\n",
    "\t\t\tclf_recall.append(recall_score(y_test, pred))\n",
    "\t\t\tclf_f1.append(f1_score(y_test, pred))\n",
    "\t\t\tclf_roc_auc.append(roc_auc_score(y_test, pred))\n",
    "\t\t\tclf_cm.append(confusion_matrix(y_test, pred))\n",
    "\t\t\tclf_cr.append(classification_report(y_test, pred, target_names = ['negative', 'positive'], digits = 6))\n",
    "\n",
    "\t\t\tprint('Results evaluated!')\n",
    "\t\t\tprint('Evaluation time: ', round(time()-t, 3), 's\\n')\n",
    "\n",
    "\t\treturn clf_accuracy, clf_precision, clf_recall, clf_f1, clf_roc_auc, clf_cm, clf_cr\n",
    "\n",
    "\tdef holdoutStrategy(self, reviews_df_preprocessed, do_pickle, do_train_data):\n",
    "\t\tprint('\\nHoldout Strategy...\\n')\n",
    "\n",
    "\t\tif do_train_data:\n",
    "\t\t\tX_train, X_test, y_train, y_test = self.trainTestSplit(reviews_df_preprocessed)\n",
    "\t\t\tpipeline, model = self.trainData(X_train, y_train, self.clf)\n",
    "\n",
    "\t\tif do_pickle:\n",
    "\t\t\twith open('pickled/features_train.pickle', 'wb') as features_train:\n",
    "\t\t\t\tpickle.dump(X_train, features_train)\n",
    "\t\t\twith open('pickled/features_test.pickle', 'wb') as features_test:\n",
    "\t\t\t\tpickle.dump(X_test, features_test)\n",
    "\t\t\twith open('pickled/labels_train.pickle', 'wb') as labels_train:\n",
    "\t\t\t\tpickle.dump(y_train, labels_train)\n",
    "\t\t\twith open('pickled/labels_test.pickle', 'wb') as labels_test:\n",
    "\t\t\t\tpickle.dump(y_test, labels_test)\n",
    "\t\t\twith open('pickled/pipeline_holdout.pickle', 'wb') as pipeline_holdout:\n",
    "\t\t\t\tpickle.dump(pipeline, pipeline_holdout)\n",
    "\t\t\twith open('pickled/model_holdout.pickle', 'wb') as model_holdout:\n",
    "\t\t\t\tpickle.dump(model, model_holdout)\n",
    "\n",
    "\t\twith open('pickled/features_train.pickle', 'rb') as features_train:\n",
    "\t\t\tX_train = pickle.load(features_train)\n",
    "\t\twith open('pickled/features_test.pickle', 'rb') as features_test:\n",
    "\t\t\tX_test = pickle.load(features_test)\n",
    "\t\twith open('pickled/labels_train.pickle', 'rb') as labels_train:\n",
    "\t\t\ty_train = pickle.load(labels_train)\n",
    "\t\twith open('pickled/labels_test.pickle', 'rb') as labels_test:\n",
    "\t\t\ty_test = pickle.load(labels_test)\n",
    "\t\twith open('pickled/pipeline_holdout.pickle', 'rb') as pipeline_holdout:\n",
    "\t\t\tpipeline = pickle.load(pipeline_holdout)\n",
    "\t\twith open('pickled/model_holdout.pickle', 'rb') as model_holdout:\n",
    "\t\t\tmodel = pickle.load(model_holdout)\n",
    "\n",
    "\t\tprediction = self.predictData(X_test, model)\n",
    "\t\tclf_accuracy, clf_precision, clf_recall, clf_f1, clf_roc_auc, clf_cm, clf_cr = self.evaluate(y_test, prediction)\n",
    "\n",
    "\t\tif do_pickle:\n",
    "\t\t\twith open('pickled/metrics_cm_holdout.pickle', 'wb') as metrics_cm:\n",
    "\t\t\t\tpickle.dump(clf_cm, metrics_cm)\n",
    "\t\t\twith open('pickled/metrics_cr_holdout.pickle', 'wb') as metrics_cr:\n",
    "\t\t\t\tpickle.dump(clf_cr, metrics_cr)\n",
    "\n",
    "\t\tmetrics_list = {\n",
    "\t\t\t'Classifier': self.clf_names,\n",
    "\t\t\t'Accuracy': clf_accuracy,\n",
    "\t\t\t'Precision': clf_precision,\n",
    "\t\t\t'Recall': clf_recall,\n",
    "\t\t\t'F1-score': clf_f1,\n",
    "\t\t\t'ROC AUC': clf_roc_auc\n",
    "\t\t}\n",
    "\n",
    "\t\tmetrics_df = pd.DataFrame.from_dict(metrics_list)\n",
    "\n",
    "\t\tfor i in range(0, len(self.clf)):\n",
    "\t\t\tif i == 0:\n",
    "\t\t\t\tprint('======================================================\\n')\n",
    "\t\t\tprint('Evaluation metrics of Classifier ' + self.clf_names[i] + ':')\n",
    "\t\t\tprint('Confusion Matrix: \\n{}\\n'.format(clf_cm[i]))\n",
    "\t\t\tprint('Classification Report: \\n{}'.format(clf_cr[i]))\n",
    "\t\t\tprint('======================================================\\n')\n",
    "\n",
    "\t\tprint('Comparison of different metrics for the various Classifiers used:\\n')\n",
    "\t\tprint(metrics_df)\n",
    "\n",
    "\t\tif do_pickle:\n",
    "\t\t\twith open('pickled/metrics_dataframe.pickle', 'wb') as df:\n",
    "\t\t\t\tpickle.dump(metrics_df, df)\n",
    "\n",
    "\tdef crossValidationStrategy(self, reviews_df_preprocessed, do_pickle):\n",
    "\t\tprint('\\nK-Fold Cross Validation Strategy...\\n')\n",
    "\n",
    "\t\ttrain_test_indices, X, y = self.kFoldSplit(reviews_df_preprocessed)\n",
    "\n",
    "\t\taccuracy = []\n",
    "\t\tprecision = []\n",
    "\t\trecall = []\n",
    "\t\tf1 = []\n",
    "\t\troc_auc = []\n",
    "\t\tcm = []\n",
    "\n",
    "\t\tfor i in range(0, len(self.clf)):\n",
    "\t\t\taccuracy.append([])\n",
    "\t\t\tprecision.append([])\n",
    "\t\t\trecall.append([])\n",
    "\t\t\tf1.append([])\n",
    "\t\t\troc_auc.append([])\n",
    "\t\t\tcm.append(np.zeros((2,2), dtype = 'int32'))\n",
    "\n",
    "\t\tfor train_idx, test_idx in train_test_indices:\n",
    "\t\t\tX_train, y_train = X[train_idx], y[train_idx]\n",
    "\t\t\tX_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "\t\t\t_, model = self.trainData(X_train, y_train, self.clf)\n",
    "\t\t\tprediction = self.predictData(X_test, model)\n",
    "\t\t\tclf_accuracy, clf_precision, clf_recall, clf_f1, clf_roc_auc, clf_cm, _ = self.evaluate(y_test, prediction)\n",
    "\n",
    "\t\t\tfor j in range(0, len(self.clf)):\n",
    "\t\t\t\taccuracy[j].append(clf_accuracy[j])\n",
    "\t\t\t\tprecision[j].append(clf_precision[j])\n",
    "\t\t\t\trecall[j].append(clf_recall[j])\n",
    "\t\t\t\tf1[j].append(clf_f1[j])\n",
    "\t\t\t\troc_auc[j].append(clf_roc_auc[j])\n",
    "\t\t\t\tcm[j] += clf_cm[j]\n",
    "\n",
    "\t\tacc = []\n",
    "\t\tprec = []\n",
    "\t\trec = []\n",
    "\t\tf1_score = []\n",
    "\t\tauc = []\n",
    "\t\tfor i in range(0, len(self.clf)):\n",
    "\t\t\tif i == 0:\n",
    "\t\t\t\tprint('======================================================\\n')\n",
    "\t\t\tprint('Evaluation metrics of Classifier ' + self.clf_names[i] + ':')\n",
    "\t\t\tprint('Accuracy: {}'.format(np.mean(accuracy[i])))\n",
    "\t\t\tprint('Precision: {}'.format(np.mean(precision[i])))\n",
    "\t\t\tprint('Recall: {}'.format(np.mean(recall[i])))\n",
    "\t\t\tprint('F1-score: {}'.format(np.mean(f1[i])))\n",
    "\t\t\tprint('ROC AUC: {}'.format(np.mean(roc_auc[i])))\n",
    "\t\t\tprint('Confusion Matrix: \\n{}\\n'.format(cm[i]))\n",
    "\t\t\tprint('======================================================\\n')\n",
    "\t\t\tacc.append(np.mean(accuracy[i]))\n",
    "            prec.append(np.mean(precision[i]))\n",
    "\t\t\trec.append(np.mean(recall[i]))\n",
    "\t\t\tf1_score.append(np.mean(f1[i]))\n",
    "\t\t\tauc.append(np.mean(roc_auc[i]))\n",
    "\n",
    "\t\tmetrics_list = {\n",
    "\t\t\t'Classifier': self.clf_names,\n",
    "\t\t\t'Accuracy': clf_accuracy,\n",
    "\t\t\t'Precision': clf_precision,\n",
    "\t\t\t'Recall': clf_recall,\n",
    "\t\t\t'F1-score': clf_f1,\n",
    "\t\t\t'ROC AUC': clf_roc_auc\n",
    "\t\t}\n",
    "\n",
    "\t\tmetrics_df = pd.DataFrame.from_dict(metrics_list)\n",
    "\n",
    "\t\tprint('Comparison of different metrics for the various Classifiers used:\\n')\n",
    "\t\tprint(metrics_df)\n",
    "\n",
    "\t\tif do_pickle:\n",
    "\t\t\twith open('pickled/metrics_dataframe_kfold.pickle', 'wb') as df_kfold:\n",
    "\t\t\t\tpickle.dump(metrics_df, df_kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9496d344",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "class Utility:\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tself.sentiment = SentimentAnalyzer()\n",
    "\t\tself.clf = self.sentiment.clf\n",
    "\n",
    "\tdef classifiersVsFeatures(self):\n",
    "\t\twith open('pickled/features_train.pickle', 'rb') as features_train:\n",
    "\t\t\tX_train = pickle.load(features_train)\n",
    "\t\twith open('pickled/features_test.pickle', 'rb') as features_test:\n",
    "\t\t\tX_test = pickle.load(features_test)\n",
    "\t\twith open('pickled/labels_train.pickle', 'rb') as labels_train:\n",
    "\t\t\ty_train = pickle.load(labels_train)\n",
    "\t\twith open('pickled/labels_test.pickle', 'rb') as labels_test:\n",
    "\t\t\ty_test = pickle.load(labels_test)\n",
    "\n",
    "\t\tnum_features = [10000, 50000, 100000, 500000, 1000000]\n",
    "\t\t\n",
    "\t\tacc = []\n",
    "\t\tfor i in range(0, len(self.clf)):\n",
    "\t\t\tacc.append([])\n",
    "\n",
    "\t\tfor k in num_features:\n",
    "\t\t\t_, model = self.sentiment.trainData(X_train, y_train, self.clf, k)\n",
    "\t\t\tprediction = self.sentiment.predictData(X_test, model)\n",
    "\t\t\tclf_metrics = self.sentiment.evaluate(y_test, prediction)\n",
    "\n",
    "\t\t\tfor j in range(0, len(self.clf)):\n",
    "\t\t\t\tprint(clf_metrics[0][j])\n",
    "\t\t\t\tacc[j].append(clf_metrics[0][j]) # Append the accuracy of the classifier for each k\n",
    "\n",
    "\t\tdata = []\n",
    "\t\tfor i in range (0, len(self.clf)):\n",
    "\t\t\tdata.append({'x': num_features, 'y': acc[i]})\n",
    "\n",
    "\t\treturn data\n",
    "\n",
    "\tdef showTopFeatures(self, pipeline, n = 20):\n",
    "\t\tvectorizer = pipeline.named_steps['vect']\n",
    "\t\tclf = pipeline.named_steps['clf']\n",
    "\t\tfeature_names = vectorizer.get_feature_names()\n",
    "\n",
    "\t\tcoefs = sorted(zip(clf.coef_[0], feature_names), reverse = True)\n",
    "\t\ttopn = zip(coefs[:n], coefs[: -(n+1): -1])\n",
    "\t\t\n",
    "\t\ttop_features = []\n",
    "\t\tfor (coef_p, feature_p), (coef_n, feature_n) in topn:\n",
    "\t\t\ttop_features.append('{:0.4f}{: >25}    {:0.4f}{: >25}'.format(coef_p, feature_p, coef_n, feature_n))\n",
    "\n",
    "\t\treturn '\\n'.join(top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5a3bcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your review:\n",
      "product is bad\n",
      "\n",
      "Predicted sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def analyzeVisualize(sentiment):\n",
    "\twith open('pickled/pipeline_holdout.pickle', 'rb') as pipeline_holdout:\n",
    "\t\tpipeline = pickle.load(pipeline_holdout)\n",
    "\twith open('pickled/metrics_cm_holdout.pickle', 'rb') as metrics_cm:\n",
    "\t\tclf_cm = pickle.load(metrics_cm)\n",
    "\twith open('pickled/metrics_cr_holdout.pickle', 'rb') as metrics_cr:\n",
    "\t\tclf_cr = pickle.load(metrics_cr)\n",
    "\twith open('pickled/metrics_dataframe.pickle', 'rb') as df:\n",
    "\t\tmetrics_df = pickle.load(df)\n",
    "\twith open('pickled/metrics_dataframe_kfold.pickle', 'rb') as df:\n",
    "\t\tmetrics_df_kfold = pickle.load(df)\n",
    "\n",
    "\tclf_svc = pipeline[2]\n",
    "\tclf_names = sentiment.clf_names\n",
    "\tlabels = ['negative', 'positive']\n",
    "\n",
    "\tvisualizer = DataVisualizer()\n",
    "\n",
    "\tfor cm, cr, name in zip(clf_cm, clf_cr, clf_names):\n",
    "\t\tvisualizer.plotConfusionMatrix(cm, labels, name)\n",
    "\t\tvisualizer.plotClassificationReport(cr, labels, name)\n",
    "\t\n",
    "\tmetrics_df.rename(columns = {\"Accuracy\": \"value_Accuracy\", \"Precision\": \"value_Precision\", \"Recall\": \"value_Recall\", \"F1-score\": \"value_F1-score\", \"ROC AUC\": \"value_ROC AUC\"}, inplace = True)\n",
    "\tmetrics_df['id'] = metrics_df.index\n",
    "\tmetrics_df_long = pd.wide_to_long(metrics_df, stubnames = 'value', i = 'id', j = 'id_m', sep = '_', suffix = r'[a-zA-Z0-9_\\- ]+')\n",
    "\tmetrics_df_long['Metrics'] = metrics_df_long.index.get_level_values('id_m')\n",
    "\tvisualizer.plotClassifierPerformanceComparison(metrics_df_long, clf_names, 'Holdout')\n",
    "\t\n",
    "\tmetrics_df_kfold.rename(columns = {\"Accuracy\": \"value_Accuracy\", \"Precision\": \"value_Precision\", \"Recall\": \"value_Recall\", \"F1-score\": \"value_F1-score\", \"ROC AUC\": \"value_ROC AUC\"}, inplace = True)\n",
    "\tmetrics_df_kfold['id'] = metrics_df_kfold.index\n",
    "\tmetrics_df_kfold_long = pd.wide_to_long(metrics_df_kfold, stubnames = 'value', i = 'id', j = 'id_m', sep = '_', suffix = r'[a-zA-Z0-9_\\- ]+')\n",
    "\tmetrics_df_kfold_long['Metrics'] = metrics_df_kfold_long.index.get_level_values('id_m')\n",
    "\tvisualizer.plotClassifierPerformanceComparison(metrics_df_kfold_long, clf_names, 'K-Fold')\n",
    "\t\n",
    "\tutil = Utility()\n",
    "\n",
    "\tdata = util.classifiersVsFeatures()\n",
    "\tcolors = ['blue', 'yellow', 'red', 'green']\n",
    "\tvisualizer.plotClassifiersVsFeatures(data, clf_names, colors)\n",
    "\n",
    "\ttop_features = util.showTopFeatures(clf_svc, n = 30)\n",
    "\tprint('The 30 most informative features for both positive and negative coefficients:\\n')\n",
    "\tprint(top_features)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\tdo_pickle = False\n",
    "\tdo_train_data = False\n",
    "\tdo_fetch_data = False\n",
    "\tdo_preprocess_data = False\n",
    "\tdo_cross_validation_strategy = False\n",
    "\tdo_holdout_strategy = False\n",
    "\tdo_analyze_visualize = False\n",
    "\n",
    "\n",
    "\tPath('./pickled').mkdir(exist_ok = True)\n",
    "\tPath('./plots').mkdir(exist_ok = True)\n",
    "\n",
    "\tif do_fetch_data or do_preprocess_data or do_cross_validation_strategy or do_holdout_strategy or do_analyze_visualize:\n",
    "\t\tsentiment = SentimentAnalyzer()\n",
    "\n",
    "\tif do_fetch_data:\n",
    "\t\tsentiment.getInitialData('product_reviews.json', do_pickle)\n",
    "\n",
    "\tif do_preprocess_data:\n",
    "\t\treviews_df = pd.read_pickle('pickled/product_reviews.pickle')\n",
    "\t\tsentiment.preprocessData(reviews_df, do_pickle)\n",
    "\n",
    "\tif do_cross_validation_strategy or do_holdout_strategy:\n",
    "\t\treviews_df_preprocessed = pd.read_pickle('pickled/product_reviews_preprocessed.pickle')\n",
    "\t\tprint(reviews_df_preprocessed.isnull().values.sum()) \n",
    "\n",
    "\tif do_cross_validation_strategy:\n",
    "\t\tsentiment.crossValidationStrategy(reviews_df_preprocessed, do_pickle)\n",
    "\t\n",
    "\tif do_holdout_strategy: \n",
    "\t\tsentiment.holdoutStrategy(reviews_df_preprocessed, do_pickle, do_train_data)\n",
    "\n",
    "\tif do_analyze_visualize:\n",
    "\t\tanalyzeVisualize(sentiment)\n",
    "\t\n",
    "\twith open('pickled/model_holdout.pickle', 'rb') as model_holdout:\n",
    "\t\tmodel = pickle.load(model_holdout)\n",
    "\n",
    "\tmodel_svc = model[2][1] \n",
    "\t\n",
    "\tprint('\\nEnter your review:')\n",
    "\tuser_review = input()\n",
    "\tverdict = 'Positive' if model_svc.predict([user_review]) == 1 else 'Negative'\n",
    "\tprint('\\nPredicted sentiment: '+ verdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e653b42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
